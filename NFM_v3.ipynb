{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = True\n",
    "NewAidFlag = False\n",
    "class Config():\n",
    "    if DEV:\n",
    "        TRAIN_FILE = \"./data/train_df_v1_10_21_digit.csv\"\n",
    "        DEV_FILE = \"./data/train_df_v1_22_digit.csv\"\n",
    "    else:\n",
    "        TRAIN_FILE = \"./data/train_df_v1_10_22_digit.csv\"\n",
    "        TEST_FILE = \"./data/test_df_v1_digit.csv\"\n",
    "    \n",
    "\n",
    "    MODE = \"smape\" #logloss  smape  mae\n",
    "\n",
    "    NUM_SPLITS = 5\n",
    "    RANDOM_SEED = 2019\n",
    "    \n",
    "    if NewAidFlag:\n",
    "        FEATURE_COLS = [\n",
    "            'billing_type', \n",
    "            'ad_type',\n",
    "            'ad_type_convert_rate', \n",
    "            'material_size_convert_rate',\n",
    "            'commodity_id_convert_rate',\n",
    "            'ad_account_id','ad_industry_id',\n",
    "            'commodity_id',\n",
    "            'commodity_type',\n",
    "            'material_size',\n",
    "            'new_bid',\n",
    "            'total_aid',\n",
    "            'ad_industry_id_convert_rate',  \n",
    "            'ad_account_id_convert_rate',\n",
    "\n",
    "            'len_connectionType',\n",
    "#             'len_consuptionAbility',\n",
    "            'len_device', 'len_education', \n",
    "#             'len_gender', \n",
    "            'len_status',\n",
    "            'len_work',\n",
    "            'len_age', 'len_area', \n",
    "        ] \n",
    "#         for logloss\n",
    "#         FEATURE_COLS = ['ad_account_id',\n",
    "#                         'ad_industry_id_convert_rate', \n",
    "#            'ad_type_convert_rate',\n",
    "#            'billing_type',\n",
    "#            'commodity_type',\n",
    "#                         'material_size', \n",
    "#                         'material_size_convert_rate',\n",
    "#            'new_bid', 'total_aid',\n",
    "                        \n",
    "#                        'ad_type',\n",
    "#                         'commodity_id_convert_rate'] \n",
    "\n",
    "    else: \n",
    "#         FEATURE_COLS = [   #'last_target','ad_id','billing_type', 'weekday', 'ad_type','ad_type_convert_rate','aid_convert_rate_t'\n",
    "#             'billing_type',\n",
    "#             'weekday',\n",
    "# #             'ad_type',\n",
    "#             'ad_type_convert_rate', \n",
    "# #             'material_size_convert_rate',\n",
    "#             'commodity_id_convert_rate',\n",
    "#             'ad_account_id','ad_industry_id',\n",
    "#             'commodity_id',\n",
    "# #            'commodity_type',\n",
    "# #             'material_size',\n",
    "#            'new_bid',\n",
    "#             'total_aid',\n",
    "#             'ad_industry_id_convert_rate',  \n",
    "#             'ad_account_id_convert_rate',\n",
    "#              'aid_convert_rate',\n",
    "# #          'aid_convert_rate_max',\n",
    "#        'aid_convert_rate_mean',\n",
    "#         'aid_convert_rate_median',\n",
    "# #        'aid_convert_rate_min',\n",
    "# #             'target_max', \n",
    "#             'target_mean',\n",
    "#             'target_median',\n",
    "# #             'target_min'\n",
    "#         ] \n",
    "        FEATURE_COLS = [   #'last_target','ad_id','billing_type', 'weekday', 'ad_type','ad_type_convert_rate','aid_convert_rate_t'\n",
    "            'billing_type',\n",
    "            'weekday',\n",
    "            'ad_type',\n",
    "            'ad_type_convert_rate', \n",
    "            'material_size_convert_rate',\n",
    "            'commodity_id_convert_rate',\n",
    "            'ad_account_id','ad_industry_id',\n",
    "            'commodity_id',\n",
    "           'commodity_type',\n",
    "            'material_size',\n",
    "           'new_bid',\n",
    "            'total_aid',\n",
    "            'ad_industry_id_convert_rate',  \n",
    "            'ad_account_id_convert_rate',\n",
    "             'aid_convert_rate',\n",
    "#          'aid_convert_rate_max',\n",
    "       'aid_convert_rate_mean',\n",
    "        'aid_convert_rate_median',\n",
    "#        'aid_convert_rate_min',\n",
    "#             'target_max', \n",
    "            'target_mean',\n",
    "            'target_median',\n",
    "#             'target_min'\n",
    "#              'len_connectionType',\n",
    "# #             'len_consuptionAbility',\n",
    "#             'len_device', 'len_education', \n",
    "# #             'len_gender', \n",
    "#             'len_status',\n",
    "#             'len_work',\n",
    "            'len_age', 'len_area',            \n",
    "        ] \n",
    "        \n",
    "# #         #for logloss   'aid_convert_rate', 'aid_convert_rate_t','ad_type','ad_account_id_convert_rate', 'commodity_id_convert_rate','commodity_id', 'ad_industry_id',\n",
    "#         FEATURE_COLS = ['ad_account_id',\n",
    "#                         'ad_industry_id_convert_rate', \n",
    "#            'ad_type_convert_rate',\n",
    "#            'billing_type',\n",
    "#            'commodity_type',\n",
    "#                         'material_size', \n",
    "#                         'material_size_convert_rate',\n",
    "#            'new_bid', 'total_aid',\n",
    "                        \n",
    "# #             'len_connectionType',\n",
    "# # #             'len_consuptionAbility',\n",
    "# #        'len_device', 'len_education', \n",
    "# # #             'len_gender', \n",
    "# #             'len_status', 'len_work',\n",
    "# #              'len_age', 'len_area', \n",
    "#                        ] #,'weekday'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class FeatureDictionary(object):\n",
    "    def __init__(self,trainfile=None,testfile=None,\n",
    "                 dfTrain=None,dfTest=None,feature_cols=[]):\n",
    "        assert not ((trainfile is None) and (dfTrain is None)), \"trainfile or dfTrain at least one is set\"\n",
    "        assert not ((trainfile is not None) and (dfTrain is not None)), \"only one can be set\"\n",
    "        assert not ((testfile is None) and (dfTest is None)), \"testfile or dfTest at least one is set\"\n",
    "        assert not ((testfile is not None) and (dfTest is not None)), \"only one can be set\"\n",
    "\n",
    "        self.trainfile = trainfile\n",
    "        self.testfile = testfile\n",
    "        self.dfTrain = dfTrain\n",
    "        self.dfTest = dfTest\n",
    "\n",
    "        self.feature_cols = feature_cols\n",
    "        self.gen_feat_dict()\n",
    "\n",
    "    def gen_feat_dict(self):\n",
    "        if self.dfTrain is None:\n",
    "            dfTrain = pd.read_csv(self.trainfile)\n",
    "        else:\n",
    "            dfTrain = self.dfTrain\n",
    "        if self.dfTest is None:\n",
    "            dfTest = pd.read_csv(self.testfile)\n",
    "        else:\n",
    "            dfTest = self.dfTest\n",
    "        df = pd.concat([dfTrain,dfTest])\n",
    "        self.feat_dict = {}\n",
    "        tc = 0\n",
    "        for col in df.columns:\n",
    "            us = df[col].unique()\n",
    "            self.feat_dict[col] = dict(zip(us,range(tc,len(us)+tc)))\n",
    "            tc += len(us)\n",
    "        self.feat_dim = tc\n",
    "\n",
    "class DataParser(object):\n",
    "    def __init__(self,feat_dict):\n",
    "        self.feat_dict = feat_dict\n",
    "\n",
    "    def parse(self,infile=None,df=None,has_label=False):\n",
    "        assert not ((infile is None) and (df is None)), \"infile or df at least one is set\"\n",
    "        assert not ((infile is not None) and (df is not None)), \"only one can be set\"\n",
    "\n",
    "        if infile is None:\n",
    "            dfi = df.copy()\n",
    "        else:\n",
    "            dfi = pd.read_csv(infile)\n",
    "        \n",
    "        if has_label:\n",
    "            y = dfi['target'].values.tolist()\n",
    "            dfi = pd.DataFrame(dfi,columns=self.feat_dict.feature_cols)\n",
    "        else:\n",
    "            ids = dfi['ad_id'].values.tolist()\n",
    "            dfi = pd.DataFrame(dfi,columns=self.feat_dict.feature_cols)\n",
    "\n",
    "        dfv = dfi.copy()\n",
    "        for col in dfi.columns:\n",
    "            dfi[col] = dfi[col].map(self.feat_dict.feat_dict[col])\n",
    "            dfv[col] = 1.\n",
    "\n",
    "        xi = dfi.values.tolist()\n",
    "        xv = dfv.values.tolist()\n",
    "\n",
    "        if has_label:\n",
    "            return xi,xv,y\n",
    "        else:\n",
    "            return xi,xv,ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AFM(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, feature_size, field_size,\n",
    "                 embedding_size=8,\n",
    "                 deep_layers=[32, 32], deep_init_size = 50,\n",
    "                 dropout_deep=[0.5, 0.5, 0.5],\n",
    "                 deep_layer_activation=tf.nn.relu,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 learning_rate=0.001, optimizer=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                greater_is_better=True,\n",
    "                 use_inner=True):\n",
    "        assert loss_type in [\"logloss\", \"mse\", \"smape\", \"mae\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.feature_size = feature_size\n",
    "        self.field_size = field_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.deep_layers = deep_layers\n",
    "        self.deep_init_size = deep_init_size\n",
    "        self.dropout_dep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layer_activation\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result,self.valid_result = [],[]\n",
    "\n",
    "        self.use_inner = use_inner\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.feat_index = tf.placeholder(tf.int32,\n",
    "                                             shape=[None,None],\n",
    "                                             name='feat_index')\n",
    "            self.feat_value = tf.placeholder(tf.float32,\n",
    "                                           shape=[None,None],\n",
    "                                           name='feat_value')\n",
    "\n",
    "            self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name='dropout_deep_deep')\n",
    "            self.train_phase = tf.placeholder(tf.bool,name='train_phase')\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # Embeddings\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N * F * K\n",
    "            feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1])\n",
    "            self.embeddings = tf.multiply(self.embeddings,feat_value) # N * F * K\n",
    "\n",
    "            # first order term\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights['feature_bias'], self.feat_index)\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)\n",
    "\n",
    "            # second order term\n",
    "            # sum-square-part\n",
    "            self.summed_features_emb = tf.reduce_sum(self.embeddings, 1)  # None * k\n",
    "            self.summed_features_emb_square = tf.square(self.summed_features_emb)  # None * K\n",
    "\n",
    "            # squre-sum-part\n",
    "            self.squared_features_emb = tf.square(self.embeddings)\n",
    "            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K\n",
    "\n",
    "            # second order\n",
    "            self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb)\n",
    "            \n",
    "            # Deep component   self.y_deep = tf.layers.batch_normalization(self.y_deep, training=self.train_phase)\n",
    "            self.y_deep = self.y_second_order\n",
    "            #self.y_deep = tf.layers.batch_normalization(self.y_deep, training=self.train_phase)\n",
    "            for i in range(0, len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[\"layer_%d\" % i]), self.weights[\"bias_%d\" % i])\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[i + 1])\n",
    "                #self.y_deep = tf.layers.batch_normalization(self.y_deep, training=self.train_phase)\n",
    "\n",
    "            # bias\n",
    "            self.y_bias = self.weights['bias'] * tf.ones_like(self.label)\n",
    "            # out\n",
    "            self.out = tf.add_n([tf.reduce_sum(self.y_first_order,axis=1,keep_dims=True),\n",
    "                                 tf.reduce_sum(self.y_deep,axis=1,keep_dims=True),\n",
    "                                 self.y_bias])\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "            elif self.loss_type == \"smape\":\n",
    "                self.loss = 2.0 * tf.reduce_sum(tf.abs(self.label - self.out) / (tf.abs(self.label) + tf.abs(self.out) + 0.00001)) * 100\n",
    "            elif self.loss_type == \"mae\":\n",
    "                self.loss = tf.reduce_sum(tf.abs(self.label - self.out)) \n",
    "                \n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
    "                    self.loss)\n",
    "\n",
    "            #init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        #embeddings\n",
    "        weights['feature_embeddings'] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='feature_embeddings')\n",
    "        weights['feature_bias'] = tf.Variable(tf.random_normal([self.feature_size,1],0.0,1.0),name='feature_bias')\n",
    "        weights['bias'] = tf.Variable(tf.constant(0.1),name='bias')\n",
    "\n",
    "        #deep layers\n",
    "        num_layer = len(self.deep_layers)\n",
    "        input_size = self.embedding_size\n",
    "        glorot = np.sqrt(2.0/(input_size + self.deep_layers[0]))\n",
    "\n",
    "        weights['layer_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(input_size,self.deep_layers[0])),dtype=np.float32)\n",
    "        weights['bias_0'] = tf.Variable(\n",
    "            np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32)\n",
    "\n",
    "        for i in range(1,num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[i]))\n",
    "            weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "            weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def get_batch(self,Xi,Xv,y,batch_size,index):\n",
    "        start = index * batch_size\n",
    "        end = (index + 1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end],Xv[start:end],[[y_] for y_ in y[start:end]]\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "    def smape(self,y_true, y_pred):\n",
    "        return 2.0 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100\n",
    "    \n",
    "    def predict(self, Xi, Xv,y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_deep: [1.0] * len(self.dropout_dep),\n",
    "                     self.train_phase: False}\n",
    "\n",
    "        out = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "        return out\n",
    "\n",
    "    def fit_on_batch(self,Xi,Xv,y):\n",
    "        feed_dict = {self.feat_index:Xi,\n",
    "                     self.feat_value:Xv,\n",
    "                     self.label:y,\n",
    "                     self.dropout_keep_deep:self.dropout_dep,\n",
    "                     self.train_phase:True}\n",
    "\n",
    "        loss,opt = self.sess.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self, Xi_train, Xv_train, y_train,\n",
    "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "\n",
    "        has_valid = Xv_valid is not None\n",
    "        best_loss = 200\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "                self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "            if has_valid:\n",
    "                y_valid = np.array(y_valid).reshape((-1,1))\n",
    "                out = self.predict(Xi_valid, Xv_valid, y_valid)\n",
    "                if self.loss_type=='logloss':\n",
    "                    print(\"epoch: {0}, train logloss: {1:.6f}\".format(epoch + 1, roc_auc_score(y_valid,out)))\n",
    "                else:\n",
    "                    loss = self.smape(y_valid, out)\n",
    "                    best_loss = min(best_loss,loss)\n",
    "                    print(\"epoch: {0}, train loss: {1:.6f}\".format(epoch + 1, loss))\n",
    "        return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "config = Config()\n",
    "def smape(y_true, y_pred):\n",
    "    return 2.0 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100\n",
    "def get_dev(dev,old_aid_set,NewAidFlag):\n",
    "    old_index = []\n",
    "    new_index = []\n",
    "    _dev = dev.copy()\n",
    "    for index, row in tqdm(_dev.iterrows()):\n",
    "        if row['ad_id'] in old_aid_set:\n",
    "            old_index.append(index)\n",
    "        else:\n",
    "            new_index.append(index)\n",
    "    if NewAidFlag:\n",
    "        return _dev.loc[new_index]\n",
    "    else:\n",
    "        return _dev.loc[old_index]\n",
    "    \n",
    "def load_data():\n",
    "    dfTrain = pd.read_csv(config.TRAIN_FILE)\n",
    "    print(dfTrain.shape)\n",
    "    if DEV:\n",
    "        dfDev = pd.read_csv(config.DEV_FILE)\n",
    "        print(dfDev.shape)\n",
    "        if config.MODE == 'logloss':\n",
    "            old_aid_set = set(dfTrain.ad_id)\n",
    "            if NewAidFlag:\n",
    "                dfDev = get_dev(dfDev,old_aid_set,NewAidFlag)\n",
    "                dfDev.reset_index(drop=True,inplace=True)\n",
    "            else:\n",
    "                dfDev = get_dev(dfDev,old_aid_set,NewAidFlag)\n",
    "                dfDev.reset_index(drop=True,inplace=True)\n",
    "            dfDev=dfDev[dfDev['target'] >= 1]\n",
    "            dfDev.reset_index(drop=True,inplace=True)\n",
    "            dfDev['target'] = dfDev.target.apply(lambda x : 1 if x <= 1 else 0)\n",
    "        else:\n",
    "            old_aid_set = set(dfTrain.ad_id)\n",
    "            if NewAidFlag:\n",
    "                dfDev = get_dev(dfDev,old_aid_set,NewAidFlag)\n",
    "                dfDev.reset_index(drop=True,inplace=True)\n",
    "            else:\n",
    "                dfDev = get_dev(dfDev,old_aid_set,NewAidFlag)\n",
    "                dfDev.reset_index(drop=True,inplace=True)\n",
    "            dfDev=dfDev[dfDev['target'] >= 1]\n",
    "            dfDev.reset_index(drop=True,inplace=True)\n",
    "        print(dfDev.shape)\n",
    "    else:\n",
    "        dfTest = pd.read_csv(config.TEST_FILE)\n",
    "    if config.MODE == 'logloss':\n",
    "        dfTrain['target'] = dfTrain.target.apply(lambda x : 1 if x <= 1 else 0)\n",
    "    else:\n",
    "        dfTrain['target'] = dfTrain.target.apply(lambda x : 1 if x <= 1 else x)\n",
    "#         pass\n",
    "\n",
    "    cols = config.FEATURE_COLS\n",
    "\n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain['target'].values\n",
    "    \n",
    "    if DEV:\n",
    "        X_dev = dfDev[cols].values\n",
    "        y_dev = dfDev['target'].values\n",
    "    else:\n",
    "        X_test = dfTest[cols].values\n",
    "        \n",
    "    if DEV:\n",
    "        return dfTrain,dfDev,X_train,y_train,X_dev,y_dev\n",
    "    else:\n",
    "        return dfTrain,dfTest,X_train,y_train,X_test\n",
    "\n",
    "def run_dev_model_nfm(dfTrain,dfDev,pnn_params):\n",
    "    print('run_dev_model_nfm')\n",
    "    fd = FeatureDictionary(dfTrain=dfTrain,\n",
    "                           dfTest=dfDev,\n",
    "                           feature_cols=config.FEATURE_COLS)\n",
    "    data_parser = DataParser(feat_dict= fd)\n",
    "    Xi_train,Xv_train,y_train = data_parser.parse(df=dfTrain,has_label=True)\n",
    "    Xi_dev,Xv_dev,y_dev = data_parser.parse(df=dfDev,has_label=True)\n",
    "    pnn_params['feature_size'] = fd.feat_dim\n",
    "    pnn_params['field_size'] = len(Xi_train[0])\n",
    "    nfm = AFM(**pnn_params)\n",
    "    loss = nfm.fit(Xi_train, Xv_train, y_train, Xi_dev,Xv_dev,y_dev)\n",
    "    y_dev = np.array(y_dev).reshape((-1,1))\n",
    "    out = nfm.predict(Xi_dev, Xv_dev, y_dev)    \n",
    "    return out\n",
    "\n",
    "def run_test_model_nfm(dfTrain,dfTest,pnn_params):\n",
    "    print('run_test_model_nfm')\n",
    "    fd = FeatureDictionary(dfTrain=dfTrain,\n",
    "                           dfTest=dfTest,\n",
    "                           feature_cols=config.FEATURE_COLS)\n",
    "    data_parser = DataParser(feat_dict= fd)\n",
    "    Xi_train,Xv_train,y_train = data_parser.parse(df=dfTrain,has_label=True)\n",
    "    Xi_test,Xv_test,_ = data_parser.parse(df=dfTest)\n",
    "    pnn_params['feature_size'] = fd.feat_dim\n",
    "    pnn_params['field_size'] = len(Xi_train[0])\n",
    "    nfm = AFM(**pnn_params)\n",
    "    loss = nfm.fit(Xi_train, Xv_train, y_train, Xi_train[:10000],Xv_train[:10000],y_train[:10000])\n",
    "    y_test = np.zeros(len(Xi_test))\n",
    "    y_test = np.array(y_test).reshape((-1,1))\n",
    "    y_test = nfm.predict(Xi_test, Xv_test, y_test)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(677738, 37)\n",
      "(57380, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57380it [00:02, 21467.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14508, 37)\n",
      "run_dev_model_nfm\n",
      "#params: 3813595\n",
      "epoch: 1, train loss: 65.393220\n",
      "epoch: 2, train loss: 63.044441\n",
      "epoch: 3, train loss: 61.821918\n",
      "epoch: 4, train loss: 62.207769\n",
      "epoch: 5, train loss: 61.336822\n",
      "epoch: 6, train loss: 61.137027\n",
      "epoch: 7, train loss: 60.655239\n",
      "epoch: 8, train loss: 60.687061\n",
      "epoch: 9, train loss: 60.377623\n",
      "epoch: 10, train loss: 60.890567\n",
      "epoch: 11, train loss: 60.113732\n",
      "epoch: 12, train loss: 60.299488\n",
      "epoch: 13, train loss: 59.907301\n",
      "epoch: 14, train loss: 60.018637\n",
      "epoch: 15, train loss: 60.185020\n"
     ]
    }
   ],
   "source": [
    "if config.MODE == 'logloss':\n",
    "    pnn_params = {\n",
    "        \"embedding_size\":16,\n",
    "        \"deep_layers\":[256,128],\n",
    "        \"dropout_deep\":[0.6,0.6,0.6], # old ad_id 0.8\n",
    "        \"deep_layer_activation\":tf.nn.relu,\n",
    "        \"epoch\":12,#old ad 35    new ad 5\n",
    "        \"batch_size\":1024,\n",
    "        \"learning_rate\":0.0005,   #old ad 0.001\n",
    "        \"optimizer\":\"adam\",\n",
    "        \"loss_type\":config.MODE, #\"logloss\",\"smape\"\n",
    "        \"batch_norm\":1,\n",
    "        \"batch_norm_decay\":0.995,\n",
    "        \"verbose\":True,\n",
    "        \"random_seed\":config.RANDOM_SEED,\n",
    "        \"deep_init_size\":50,\n",
    "        \"use_inner\":False\n",
    "    }\n",
    "else:\n",
    "    if NewAidFlag:\n",
    "        #new aid\n",
    "        pnn_params = {\n",
    "            \"embedding_size\":8,\n",
    "            \"deep_layers\":[128,64],\n",
    "            \"dropout_deep\":[0.8,0.8,0.8], # old ad_id 0.8\n",
    "            \"deep_layer_activation\":tf.nn.relu,\n",
    "            \"epoch\":10,#old ad 35    new ad 5\n",
    "            \"batch_size\":1024,\n",
    "            \"learning_rate\":0.001,   #old ad 0.001\n",
    "            \"optimizer\":\"adam\",\n",
    "            \"loss_type\":config.MODE, #\"logloss\",\"smape\"\n",
    "            \"batch_norm\":1,\n",
    "            \"batch_norm_decay\":0.995,\n",
    "            \"verbose\":True,\n",
    "            \"random_seed\":config.RANDOM_SEED,\n",
    "            \"deep_init_size\":50,\n",
    "            \"use_inner\":False\n",
    "        }\n",
    "    else:\n",
    "        #lod aid\n",
    "        pnn_params = {\n",
    "            \"embedding_size\":16,\n",
    "            \"deep_layers\":[256,128],\n",
    "            \"dropout_deep\":[0.7,0.7,0.7], # old ad_id 0.8\n",
    "            \"deep_layer_activation\":tf.nn.relu,\n",
    "            \"epoch\":15,#old ad 35    new ad 5\n",
    "            \"batch_size\":1024,\n",
    "            \"learning_rate\":0.003,   #old ad 0.001\n",
    "            \"optimizer\":\"adam\",\n",
    "            \"loss_type\":config.MODE, #\"logloss\",\"smape\"\n",
    "            \"batch_norm\":1,\n",
    "            \"batch_norm_decay\":0.995,\n",
    "            \"verbose\":True,\n",
    "            \"random_seed\":config.RANDOM_SEED,\n",
    "            \"deep_init_size\":50,\n",
    "            \"use_inner\":False\n",
    "        }\n",
    "if DEV:\n",
    "    dfTrain,dfDev,X_train, y_train,X_dev,y_dev = load_data()\n",
    "    predict_dev = run_dev_model_nfm(dfTrain,dfDev,pnn_params)\n",
    "else:\n",
    "    dfTrain,dfTest,X_train,y_train,X_test = load_data()\n",
    "    predict_test = run_test_model_nfm(dfTrain,dfTest,pnn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smape test_dev 59.09294105321281\n"
     ]
    }
   ],
   "source": [
    "test_dev = pd.DataFrame(dfDev,columns=['ad_id','target'])\n",
    "test_dev['pred'] = predict_dev\n",
    "test_dev['pred'] = test_dev['pred'].apply(lambda x : 1 if x < 1 else x)\n",
    "print('smape test_dev',smape(test_dev['target'],test_dev['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14508.000000</td>\n",
       "      <td>14508.000000</td>\n",
       "      <td>14508.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>129693.615247</td>\n",
       "      <td>19.490419</td>\n",
       "      <td>19.230201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>74940.405202</td>\n",
       "      <td>115.204132</td>\n",
       "      <td>89.493490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>64690.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.003653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>129944.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.903118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>194589.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.497584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>259530.000000</td>\n",
       "      <td>7859.000000</td>\n",
       "      <td>2713.360596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ad_id        target          pred\n",
       "count   14508.000000  14508.000000  14508.000000\n",
       "mean   129693.615247     19.490419     19.230201\n",
       "std     74940.405202    115.204132     89.493490\n",
       "min         6.000000      1.000000      1.000000\n",
       "25%     64690.000000      1.000000      1.003653\n",
       "50%    129944.500000      3.000000      2.903118\n",
       "75%    194589.500000      8.000000      9.497584\n",
       "max    259530.000000   7859.000000   2713.360596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dev.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
